{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2e68a0-d2ea-4999-8ed0-68505624e248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f877a66efe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed982964-70b7-4fa9-8e44-be16b5dd3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "from math import pi\n",
    "from typing import Any, Callable, Dict, Optional, Type\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub\n",
    "from huggingface_hub import notebook_login\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from sb3_contrib import QRDQN, TQC\n",
    "from stable_baselines3 import A2C, DQN, PPO, SAC, TD3\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da05eb-32ae-4dee-84d1-34c5530e356b",
   "metadata": {},
   "source": [
    "# Part 1: The Importance of Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc404a28-b09e-4ede-a6a3-307c40122b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'Pendulum-v1'\n",
    "# Env used only for evaluation\n",
    "eval_envs = make_vec_env(env_id, n_envs=10)\n",
    "# 4000 training timesteps\n",
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad4a6-72dc-4fda-ac0c-f47b85fd563b",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7bb9e-2617-4a11-a564-3327266dfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "ppo_model = PPO('MlpPolicy', env_id, seed=0, verbose=0).learn(budget_pendulum)\n",
    "t1 = time.time()\n",
    "print(f'Trained in {t1-t0:.2f} s ({budget_pendulum/(t1-t0):.1f} steps/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1b86b-2b91-4411-9930-550fd7af9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f'Episode reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126247c-40dc-45be-a01b-fc2ef14d3544",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3e1ed-d60f-4376-bb41-98071e5fb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "a2c_model = A2C('MlpPolicy', env_id, seed=0, verbose=0).learn(budget_pendulum)\n",
    "t1 = time.time()\n",
    "print(f'Trained in {t1-t0:.2f} s ({budget_pendulum/(t1-t0):.1f} steps/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d8a08-a124-4dd6-996c-76a60a68f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f'Episode reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0fa0b-f6da-4430-9d36-70f2a28f925f",
   "metadata": {},
   "source": [
    "### Training longer PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee26b3-caca-41d3-b588-d955d875dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_budget = 10 * budget_pendulum\n",
    "\n",
    "t0 = time.time()\n",
    "ppo_model = PPO('MlpPolicy', env_id, seed=0, verbose=0).learn(new_budget)\n",
    "t1 = time.time()\n",
    "print(f'Trained in {t1-t0:.2f} s ({new_budget/(t1-t0):.1f} steps/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f0835-b95f-4ebb-a50d-77a0ece33f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f'Episode reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3c30b-387e-4e0d-bf42-8467eae51bc6",
   "metadata": {},
   "source": [
    "### Tuning PPO hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013a3fd-2efb-4d72-b912-e06582f78611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_params = {\n",
    "    'gamma': 0.9,\n",
    "    'use_sde': True,\n",
    "    'sde_sample_freq': 4,\n",
    "    'learning_rate': 1e-3,\n",
    "}\n",
    "budget = 50000\n",
    "\n",
    "t0 = time.time()\n",
    "ppo_tuned_model = PPO(\n",
    "    'MlpPolicy', \n",
    "    env_id, \n",
    "    seed=0, \n",
    "    verbose=1, \n",
    "    **tuned_params).learn(budget, log_interval=5)\n",
    "t1 = time.time()\n",
    "print(f'Trained in {t1-t0:.2f} s ({budget/(t1-t0):.1f} steps/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58910c3a-1673-46ac-bbf2-b6a0e5449956",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"Tuned PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bc503-774f-4de9-b388-cde71bf19119",
   "metadata": {},
   "source": [
    "## Part III: Automatic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ab0a0-2b2a-4719-89ac-26800aaef754",
   "metadata": {},
   "source": [
    "**Hyperparameter sampling functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5b2641-3862-4a81-ae15-31837694d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for A2C hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial.\n",
    "        \n",
    "    Returns:\n",
    "        (Dict[str, Any]): Sampled hyperparameters for the given trial.\n",
    "    \n",
    "    \"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float('gamma', 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 5.0, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int('exponent_n_steps', 3, 10)\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.00001, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    \n",
    "    trial.set_user_attr('gamma_', gamma)\n",
    "    trial.set_user_attr('n_steps', n_steps)\n",
    "    \n",
    "    net_arch = [\n",
    "        {'pi': [64], 'vf': [64]}\n",
    "        if net_arch == 'tiny'\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "    \n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    return {\n",
    "        'n_steps': n_steps,\n",
    "        'gamma': gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dfe1ea-c375-4758-b258-c565cdc302da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for PPO hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial.\n",
    "        \n",
    "    Returns:\n",
    "        (Dict[str, Any]): Sampled hyperparameters for the given trial.\n",
    "    \n",
    "    \"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float('gamma', 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 5.0, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int('exponent_n_steps', 3, 10)\n",
    "    \n",
    "    gae_lambda = 1 - trial.suggest_float('gae_lambda', 0.0001, 0.2, log=True)    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.00001, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    \n",
    "    trial.set_user_attr('gamma_', gamma)\n",
    "    trial.set_user_attr('n_steps', n_steps)\n",
    "    \n",
    "    net_arch = [\n",
    "        {'pi': [64], 'vf': [64]}\n",
    "        if net_arch == 'tiny'\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "    \n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    return {\n",
    "        'n_steps': n_steps,\n",
    "        'gamma': gamma,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def init_ppo_params() -> Dict[str, Any]:\n",
    "    \"\"\"Return default parameter values for an initial trial\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "        'n_steps': 128,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'learning_rate': 2.5e-4,\n",
    "        'max_grad_norm': 0.5,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': {'pi': [64], 'vf': [64]},\n",
    "            'activation_fn': nn.ReLU},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff258-2665-492e-a252-71bc1e36985f",
   "metadata": {},
   "source": [
    "**Optuna eval callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f0f1df-9cbc-407c-bd45-cf9e7a683b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f4fac-ef38-48a0-9e0c-bae6b7c3a66d",
   "metadata": {},
   "source": [
    "**Training objective function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776f93c7-b4b3-4fb7-a597-774a692029b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model_alg: Type[BaseAlgorithm],\n",
    "        hyperparameters: Dict[str, Any],\n",
    "        n_timesteps: int,\n",
    "        save_file: Optional[str] = None) -> BaseAlgorithm:\n",
    "    \"\"\"Train a stable_baselines3 model according to a set of\n",
    "    hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        model_alg (Type[BaseAlgorithm]): A stable_baselines3 RL algorithm class.\n",
    "        hyperparameters (Dict[str, Any]): kwargs for the `model_alg` constructor.\n",
    "        n_timesteps (int): Number of training timesteps.\n",
    "        save_file (Optional[str]): If specified, save the trained model here.\n",
    "        \n",
    "    Returns:\n",
    "        model (BaseAlgorithm): A trained stable_baselines3 model.\n",
    "        \n",
    "    \"\"\"\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    # Create the RL model\n",
    "    model = model_alg(**hyperparameters)\n",
    "    \n",
    "    # Train\n",
    "    model.learn(n_timesteps, log_interval=1000000, progress_bar=True)\n",
    "    \n",
    "    if save_file is not None:\n",
    "        os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "        model.save(save_file)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2871048-49ff-4999-8d0f-200c4000131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "        trial: optuna.Trial, \n",
    "        model_alg: Type[BaseAlgorithm],\n",
    "        sample_fn: Callable[[optuna.Trial], Dict[str, Any]]) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "    \n",
    "    Args:\n",
    "        trial (optuna.Trial): Optuna trial object.\n",
    "        model_alg (Type[BaseAlgorithm]): A stable_baselines3 RL algorithm class.\n",
    "        sample_fn (Callable[[optuna.Trial], Dict[str, Any]]): Function for\n",
    "            generating hyperparameter sample suggestions.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    kwargs.update(sample_fn(trial))\n",
    "\n",
    "    # Create the RL model\n",
    "    model = model_alg(**kwargs)\n",
    "\n",
    "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "    \n",
    "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
    "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
    "    # TrialEvalCallback signature:\n",
    "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_envs, \n",
    "        trial, \n",
    "        N_EVAL_EPISODES, \n",
    "        EVAL_FREQ, \n",
    "        deterministic=True, \n",
    "        verbose=False)\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043e335-ca6a-4f06-897b-b626525eb7ee",
   "metadata": {},
   "source": [
    "### Atari hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b3664f-1e66-41d4-b5ec-b595ed96a097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 10  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 20  # Number of evaluations during the training\n",
    "N_TIMESTEPS = 250_000 # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 8\n",
    "N_EVAL_EPISODES = 10\n",
    "N_TRAIN_ENVS = 8\n",
    "TIMEOUT = int(60 * 60 * 12)  # 12 hours\n",
    "\n",
    "\n",
    "ENV_ID = \"Asteroids-v4\"\n",
    "MODEL_ALG = PPO\n",
    "HP_SAMPLER = sample_ppo_params\n",
    "INIT_SAMPLER = init_ppo_params\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"CnnPolicy\",\n",
    "    \"env\": make_vec_env(ENV_ID, n_envs=N_TRAIN_ENVS),\n",
    "    'ent_coef': 0.01,\n",
    "    'vf_coef': 0.5,\n",
    "    'clip_range': 0.1,\n",
    "    'batch_size': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5fddf-fec2-481a-b058-953ef8ae891a",
   "metadata": {},
   "source": [
    "### CartPole hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73570f4-67c8-4e2b-9675-dc53c2f86f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 10  # Maximum number of trials\n",
    "N_JOBS = 2 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 10  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 5\n",
    "N_EVAL_EPISODES = 10\n",
    "N_TRAIN_ENVS = 8\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "MODEL_ALG = A2C\n",
    "HP_SAMPLER = sample_a2c_params\n",
    "INIT_SAMPLER = init_a2c_params\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": make_vec_env(ENV_ID, n_envs=N_TRAIN_ENVS),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1f062-f18e-414f-991d-c5e4e78872a3",
   "metadata": {},
   "source": [
    "### HPO loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e7777-798c-47c6-9516-a280be66af56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 4\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "model_name = f'{MODEL_ALG}'.split('.')[-1][:-2]\n",
    "study_name = f'{ENV_ID}_{model_name}'\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler, \n",
    "    pruner=pruner, \n",
    "    direction=\"maximize\", \n",
    "    study_name=study_name)\n",
    "\n",
    "# Start with default hyperparams pulled from some other source\n",
    "study.enqueue_trial(INIT_SAMPLER())\n",
    "\n",
    "# Specify the model algorithm and hyperparameter sampler to \n",
    "# create a final objective function\n",
    "objective_final = lambda t: objective(t, MODEL_ALG, HP_SAMPLER)\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        objective_final, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig1.show()\n",
    "try:\n",
    "    fig2 = plot_param_importances(study)\n",
    "    fig2.show()\n",
    "except Exception as e:\n",
    "    print(f'Param importance plot failed with error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc1424-a9f0-45ce-a910-c0a071c3854f",
   "metadata": {},
   "source": [
    "# Retraining a study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c7e5f-1f70-42af-bc14-7a279b23c6b5",
   "metadata": {},
   "source": [
    "**Save best params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1964a7c-5943-4044-9250-286996ec0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_params.update(DEFAULT_HYPERPARAMS)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70863ce-4692-4de1-8dcb-e1ed472757ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('content/rl-baselines3-zoo/logs/best-asteroids-20230129.json', 'w') as fd:\n",
    "    json.dump(best_params, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07853cde-a09a-45b5-95c7-6f2e9bf77eed",
   "metadata": {},
   "source": [
    "**Get top K configs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981cd67-986f-4766-aca9-48d150a2e184",
   "metadata": {},
   "source": [
    "**Hackily get top 5 configs from text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8683430-0e14-4b74-9c3c-d3ca31bd1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = [\n",
    "    {'gamma': 0.060826137246005656, 'max_grad_norm': 4.512018052188801, 'exponent_n_steps': 8, 'gae_lambda': 0.0003259354802354418, 'learning_rate': 0.00013038419011988115, 'net_arch': 'small', 'activation_fn': 'tanh'},\n",
    "    {'gamma': 0.07663570395711342, 'max_grad_norm': 3.478570404779269, 'exponent_n_steps': 9, 'gae_lambda': 0.0013308385822802026, 'learning_rate': 0.00019024287008661774, 'net_arch': 'tiny', 'activation_fn': 'relu'},\n",
    "    {'gamma': 0.06856468429867885, 'max_grad_norm': 2.5653104623097125, 'exponent_n_steps': 9, 'gae_lambda': 0.0005156622747407014, 'learning_rate': 0.0002227256156467384, 'net_arch': 'small', 'activation_fn': 'tanh'},\n",
    "    {'gamma': 0.0511020782049801, 'max_grad_norm': 2.6154552042837587, 'exponent_n_steps': 10, 'gae_lambda': 0.0007317209203167054, 'learning_rate': 0.0001927384326410831, 'net_arch': 'small', 'activation_fn': 'tanh'},\n",
    "    {'gamma': 0.04389554306290792, 'max_grad_norm': 3.785866867421681, 'exponent_n_steps': 9, 'gae_lambda': 0.0010996358649999723, 'learning_rate': 0.000296739645522824, 'net_arch': 'tiny', 'activation_fn': 'relu'},\n",
    "]\n",
    "for t in top5:\n",
    "    t.update(DEFAULT_HYPERPARAMS)\n",
    "    lr = t['learning_rate']\n",
    "    t['learning_rate'] = lambda x: lr * 0.5 ** int(10 * (1 - x))\n",
    "    t['gamma'] = 1 - t['gamma']\n",
    "    t['gae_lambda'] = 1 - t['gae_lambda']\n",
    "    t['n_steps'] = int(2**t['exponent_n_steps'])\n",
    "    del t['exponent_n_steps']\n",
    "    t['net_arch'] = [\n",
    "        {'pi': [64], 'vf': [64]}\n",
    "        if t['net_arch'] == 'tiny'\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "    \n",
    "    t['activation_fn'] = {'tanh': nn.Tanh, 'relu': nn.ReLU}[t['activation_fn']]\n",
    "    t['policy_kwargs'] = {'net_arch': t['net_arch'], 'activation_fn': t['activation_fn']}\n",
    "    del t['net_arch']\n",
    "    del t['activation_fn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98937dcb-16be-475d-a57e-4635da75e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/venv/hfrl-unit3/lib/python3.10/site-packages/stable_baselines3/common/policies.py:458: UserWarning: As shared layers in the mlp_extractor are deprecated and will be removed in SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e494f4dafba0436487729b0a312464bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37dd26c05a94085985ca5b2a4a57690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bdc1b715fb46ad9c362ad756a71327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = str(MODEL_ALG).split('.')[-1][:-2].lower()\n",
    "n_steps = 10_000_000\n",
    "for i, hyperparams in enumerate(top5):\n",
    "    save_file = os.path.join(\n",
    "        'content', 'rl-baselines3-zoo', 'logs', model_name, f'{i:02}.zip')\n",
    "    train(MODEL_ALG, hyperparams, n_steps, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855c98c-42e5-49b6-94f0-be7d631c7d67",
   "metadata": {},
   "source": [
    "## Evaluate trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47525e2e-c7f7-4d8a-aa7e-7342028b0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = PPO.load('./content/rl-baselines3-zoo/logs/ppo/00.zip')\n",
    "eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc65faee-37c4-474f-8d5f-7105d7379be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/home/matt/venv/hfrl-unit3/lib/python3.10/site-packages/stable_baselines3/common/policies.py:458: UserWarning: As shared layers in the mlp_extractor are deprecated and will be removed in SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████| 5/5 [02:05<00:00, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Episode reward: 795.00 +/- 276.63', 1: 'Episode reward: 520.30 +/- 180.17', 2: 'Episode reward: 715.70 +/- 219.28', 3: 'Episode reward: 949.20 +/- 340.12', 4: 'Episode reward: 761.20 +/- 307.95'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in tqdm.tqdm(range(5), total=5):\n",
    "    model = PPO.load(f'./content/rl-baselines3-zoo/logs/ppo/{i:02}.zip')\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "    results[i] = f'Episode reward: {mean_reward:.2f} +/- {std_reward:.2f}'\n",
    "    \n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "169a70e6-5064-4a26-8ae7-a294b96013c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-0 Episode reward: 795.00 +/- 276.63\n",
      "Top-1 Episode reward: 520.30 +/- 180.17\n",
      "Top-2 Episode reward: 715.70 +/- 219.28\n",
      "Top-3 Episode reward: 949.20 +/- 340.12\n",
      "Top-4 Episode reward: 761.20 +/- 307.95\n"
     ]
    }
   ],
   "source": [
    "for k in results:\n",
    "    print(f'Top-{k}', results[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfrl-unit3",
   "language": "python",
   "name": "hfrl-unit3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
